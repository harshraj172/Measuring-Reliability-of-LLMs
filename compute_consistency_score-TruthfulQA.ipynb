{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5493d6b2",
   "metadata": {},
   "source": [
    "# Compute consistency scores of TruthfulQA\n",
    "\n",
    "**Consistency Scores**\n",
    "All of these are done on greedy and sampled\n",
    "\n",
    "* Compute consistency of questions\n",
    "* Compute answer consistency on all per model 'facebook_opt-125m', 'facebook_opt-350m', 'facebook_opt-1.3b', 'facebook_opt-2.7b' with average answer accuracy \n",
    "* Compute answer consistency per model human filtered only\n",
    "* Compute consistency of accuracate quetsions only (human v non human filtered)\n",
    "\n",
    "**Correlation scores**\n",
    "* Compute correlation of answer consistency scores with averaege answer accuracy\n",
    "* Compute correlation of question consistency scores with average answer accuracy\n",
    "* Compute correlation of question consistency with answer consistency\n",
    "\n",
    "**Addtional**\n",
    "* Analysis of consistent and inconsistent questions\n",
    "* Analysis of human filtered (what was removed and how much)\n",
    "\n",
    "```py\n",
    "scale_options = ['facebook_opt-125m', 'facebook_opt-350m', 'facebook_opt-1.3b', 'facebook_opt-2.7b']\n",
    "decoder_options = ['greedy_answers', 'sampled_answers']\n",
    "human_filtered = ['true', 'not_true']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ee76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from utils import ConsistencyScoring\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050efb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "consistency_scorer = ConsistencyScoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160642d4",
   "metadata": {},
   "source": [
    "## Consistency of questions together and by paraphrase type\n",
    "\n",
    "**ToDo**: Run on human filtered and actually answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5a76a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4493"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = pd.read_csv('../data/truthful_qa_top_6_by_pp.csv')\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1100f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO add human filtered analysis here!\n",
    "split_two = pd.read_csv('../data/truthful_qa_top_6_by_pp_split_2_manually_filtered.txt')\n",
    "split_two = split_two[split_two['keep'] != False]\n",
    "split_one = pd.read_csv('../data/truthful_qa_top_6_by_pp_split_1_manually_filtered.csv')\n",
    "human_filtered = pd.concat([split_one, split_two])\n",
    "len(human_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46eca10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add original_question as a question to ask\n",
    "original_questions = pd.DataFrame([\n",
    "    {'dataset': 'truthful_qa',\n",
    "     'passage': None,\n",
    "     'original question': question,\n",
    "     'paraphrased question': question,\n",
    "     'paraphrased generaton model': 'original',\n",
    "     'pp_score': 1.0} for question in set(questions['original question'])\n",
    "])\n",
    "len(original_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61948d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([original_questions, questions])\n",
    "human_filtered_df = pd.concat([human_filtered, original_questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9112d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['original', 'T5-finetuned', 'Prompt-text-davinci-002', 'QC'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['paraphrased generaton model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d2a2aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paraphrased generaton model\n",
       "Prompt-text-davinci-002     392\n",
       "QC                         1410\n",
       "T5-finetuned               2691\n",
       "original                    805\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('paraphrased generaton model').count()['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7375fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_consistency = {}\n",
    "# for q in tqdm(df['original question'].unique()):\n",
    "#     questions = df[df['original question'] == q]['paraphrased question']\n",
    "#     question_consistency[q] = consistency_scorer.get_score(questions)\n",
    "# q_consistency_df = pd.DataFrame([{'question': q, **v} for q, v in list(question_consistency.items())])\n",
    "# q_consistency_df.to_csv('../data/question_consistency_scores_all.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da849b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pp_type in ['T5-finetuned', 'Prompt-text-davinci-002', 'QC']:\n",
    "#     question_consistency = {}\n",
    "#     for q in tqdm(df['original question'].unique()):\n",
    "#         q_df = df[df['paraphrased generaton model'].isin([pp_type, 'original'])]\n",
    "#         questions = q_df[q_df['original question'] == q]['paraphrased question']\n",
    "#         if len(questions) < 2:\n",
    "#             continue\n",
    "#         question_consistency[q] = consistency_scorer.get_score(questions)\n",
    "#     pd.DataFrame([{'question': q, **v} for q, v in list(question_consistency.items())]).to_csv(f'../data/question_consistency_scores_{pp_type}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca36e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_consistency = {}\n",
    "# for q in tqdm(human_filtered_df['original question'].unique()):\n",
    "#     questions = human_filtered_df[human_filtered_df['original question'] == q]['paraphrased question']\n",
    "#     if len(questions) == 0:\n",
    "#         continue\n",
    "#     question_consistency[q] = consistency_scorer.get_score(questions)\n",
    "# q_consistency_df = pd.DataFrame(\n",
    "#     [{'question': q, **v} for q, v in list(question_consistency.items())]\n",
    "# )\n",
    "# q_consistency_df.to_csv('../data/question_consistency_scores_human_filtered.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0607ba33",
   "metadata": {},
   "source": [
    "## Consistency of answers\n",
    "\n",
    "* answer consistency on all per model 'facebook_opt-125m', 'facebook_opt-350m', 'facebook_opt-1.3b', 'facebook_opt-2.7b' with average answer accuracy\n",
    "* Compute answer consistency per model human filtered only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8406a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing consistency for  facebook_opt-125m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301d2488ff724ef2aeeb5e791d147c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/804 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale_options = ['facebook_opt-125m', 'facebook_opt-350m', 'facebook_opt-1.3b', 'facebook_opt-2.7b']\n",
    "decoder_options = ['greedy_answers', 'sampled_answers']\n",
    "human_filtered = ['true', 'not_true']\n",
    "\n",
    "for model in scale_options:\n",
    "    print('Computing consistency for ', model)\n",
    "    answers_df = pd.read_csv(f'../data/{model}_acc_scores.csv').fillna('')\n",
    "    answers_df =  answers_df[answers_df['paraphrased question'].isin(\n",
    "        human_filtered_df['paraphrased question'].unique()\n",
    "    )]\n",
    "    answer_consistency = {}\n",
    "    for q in tqdm(answers_df['original question'].unique()):\n",
    "        answers = answers_df[answers_df['original question'] == q]\n",
    "        answer_strings = [ans for ans in answers['greedy_answers'] if ans]\n",
    "        if len(answer_strings) < 2:\n",
    "            continue\n",
    "        answer_consistency[q] = consistency_scorer.get_score(answer_strings)\n",
    "        acc = [ac for ac in answers['greedy_answers bleu acc'] if ac != '']\n",
    "        answer_consistency[q]['avg_acc'] = sum(acc) / len(acc)\n",
    "    pd.DataFrame([{'question': q, **v} for q, v in list(answer_consistency.items())]).to_csv(f'../data/nli/{model}_answer_consistency_greedy_human_filtered.csv', index=None)\n",
    "    \n",
    "    answer_consistency = {}\n",
    "    for q in tqdm(answers_df['original question'].unique()):\n",
    "        answers = answers_df[answers_df['original question'] == q]\n",
    "        answers = answers[answers['greedy_answers bleu acc'] == 1]\n",
    "        answer_strings = [ans for ans in answers['greedy_answers'] if ans]\n",
    "        if len(answer_strings) < 2:\n",
    "            continue\n",
    "        answer_consistency[q] = consistency_scorer.get_score(answer_strings)\n",
    "        answer_consistency[q]['avg_acc'] = 1.0\n",
    "    pd.DataFrame([{'question': q, **v} for q, v in list(answer_consistency.items())]).to_csv(f'../data/nli/{model}_answer_consistency_greedy_and_accurate_human_filtered.csv', index=None)\n",
    "    \n",
    "    answer_consistency = {}\n",
    "    for q in tqdm(answers_df['original question'].unique()):\n",
    "        answers = answers_df[answers_df['original question'] == q]\n",
    "        answer_strings = [ans for ans in answers['sampled_answers'] if ans]\n",
    "        if len(answer_strings) < 2:\n",
    "            continue\n",
    "        answer_consistency[q] = consistency_scorer.get_score(answer_strings)\n",
    "        acc = [ac for ac in answers['sampled_answers bleu acc'] if ac != '']\n",
    "        answer_consistency[q]['avg_acc'] = sum(acc) / len(acc)\n",
    "    pd.DataFrame([{'question': q, **v} for q, v in list(answer_consistency.items())]).to_csv(f'../data/nli/{model}_answer_consistency_sampled_human_filtered.csv', index=None)\n",
    "    \n",
    "    answer_consistency = {}\n",
    "    for q in tqdm(answers_df['original question'].unique()):\n",
    "        answers = answers_df[answers_df['original question'] == q]\n",
    "        answers = answers[answers['sampled_answers bleu acc'] == 1]\n",
    "        answer_strings = [ans for ans in answers['sampled_answers'] if ans]\n",
    "        if len(answer_strings) < 2:\n",
    "            continue\n",
    "        answer_consistency[q] = consistency_scorer.get_score(answer_strings)\n",
    "        answer_consistency[q]['avg_acc'] = 1.0\n",
    "    pd.DataFrame([{'question': q, **v} for q, v in list(answer_consistency.items())]).to_csv(f'../data/nli/{model}_answer_consistency_sampled_and_accurate_human_filtered.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a473d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_options = ['facebook_opt-125m', 'facebook_opt-350m', 'facebook_opt-1.3b', 'facebook_opt-2.7b']\n",
    "decoder_options = ['greedy_answers', 'sampled_answers']\n",
    "human_filtered = ['true', 'not_true']\n",
    "\n",
    "for model in scale_options:\n",
    "    print('Computing consistency for ', model)\n",
    "    answers_df = pd.read_csv(f'../data/{model}_acc_scores.csv').fillna('')\n",
    "    answer_consistency = {}\n",
    "    for q in tqdm(answers_df['original question'].unique()):\n",
    "        answers = answers_df[answers_df['original question'] == q]\n",
    "        answer_strings = [ans for ans in answers['greedy_answers'] if ans]\n",
    "        if len(answer_strings) < 2:\n",
    "            continue\n",
    "        answer_consistency[q] = consistency_scorer.get_score(answer_strings)\n",
    "        acc = [ac for ac in answers['greedy_answers bleu acc'] if ac != '']\n",
    "        answer_consistency[q]['avg_acc'] = sum(acc) / len(acc)\n",
    "    pd.DataFrame([{'question': q, **v} for q, v in list(answer_consistency.items())]).to_csv(f'../data/nli/{model}_answer_consistency_greedy.csv', index=None)\n",
    "    \n",
    "    answer_consistency = {}\n",
    "    for q in tqdm(answers_df['original question'].unique()):\n",
    "        answers = answers_df[answers_df['original question'] == q]\n",
    "        answers = answers[answers['greedy_answers bleu acc'] == 1]\n",
    "        answer_strings = [ans for ans in answers['greedy_answers'] if ans]\n",
    "        if len(answer_strings) < 2:\n",
    "            continue\n",
    "        answer_consistency[q] = consistency_scorer.get_score(answer_strings)\n",
    "        answer_consistency[q]['avg_acc'] = 1.0\n",
    "    pd.DataFrame([{'question': q, **v} for q, v in list(answer_consistency.items())]).to_csv(f'../data/nli/{model}_answer_consistency_greedy_and_accurate.csv', index=None)\n",
    "    \n",
    "    answer_consistency = {}\n",
    "    for q in tqdm(answers_df['original question'].unique()):\n",
    "        answers = answers_df[answers_df['original question'] == q]\n",
    "        answer_strings = [ans for ans in answers['sampled_answers'] if ans]\n",
    "        if len(answer_strings) < 2:\n",
    "            continue\n",
    "        answer_consistency[q] = consistency_scorer.get_score(answer_strings)\n",
    "        acc = [ac for ac in answers['sampled_answers bleu acc'] if ac != '']\n",
    "        answer_consistency[q]['avg_acc'] = sum(acc) / len(acc)\n",
    "    pd.DataFrame([{'question': q, **v} for q, v in list(answer_consistency.items())]).to_csv(f'../data/nli/{model}_answer_consistency_sampled.csv', index=None)\n",
    "    \n",
    "    answer_consistency = {}\n",
    "    for q in tqdm(answers_df['original question'].unique()):\n",
    "        answers = answers_df[answers_df['original question'] == q]\n",
    "        answers = answers[answers['sampled_answers bleu acc'] == 1]\n",
    "        answer_strings = [ans for ans in answers['sampled_answers'] if ans]\n",
    "        if len(answer_strings) < 2:\n",
    "            continue\n",
    "        answer_consistency[q] = consistency_scorer.get_score(answer_strings)\n",
    "        answer_consistency[q]['avg_acc'] = 1.0\n",
    "    pd.DataFrame([{'question': q, **v} for q, v in list(answer_consistency.items())]).to_csv(f'../data/nli/{model}_answer_consistency_sampled_and_accurate.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b7e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
